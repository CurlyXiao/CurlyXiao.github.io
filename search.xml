<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>TF-IDF算法</title>
      <link href="/2021/09/15/tf-idf-suan-fa/"/>
      <url>/2021/09/15/tf-idf-suan-fa/</url>
      
        <content type="html"><![CDATA[<h2 id="1-TF-IDF算法介绍"><a href="#1-TF-IDF算法介绍" class="headerlink" title="1.TF-IDF算法介绍"></a>1.TF-IDF算法介绍</h2><p>​    <strong>TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）</strong>是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用<strong>加权技术</strong>，是一种统计方法。</p><p>​    TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。<strong>字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</strong></p><p>​    <strong>TF-IDF的主要思想是</strong>：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语对于该篇文章具有很好的代表性。</p><p>计算步骤：</p><h4 id="（1）词频（TF）"><a href="#（1）词频（TF）" class="headerlink" title="（1）词频（TF）"></a>（1）词频（TF）</h4><p>​    <strong>TF(Term Frequency)</strong>代表词条在文章中出现的频率。</p><p>​    这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。</p><p>​    TF的公式：</p><script type="math/tex; mode=display">TF_{i,j}=\frac{n_{i,j}}{\sum_k n_{k,j}}</script><p>其中，$n_{i,j}$是词条$t_i$在文章$d_j$中出现的次数,$TF_{i.j}$就是表示词条$t_i$在文档中出现的频率。</p><h4 id="（2）逆文件频率（IDF）"><a href="#（2）逆文件频率（IDF）" class="headerlink" title="（2）逆文件频率（IDF）"></a>（2）逆文件频率（IDF）</h4><p>​    <strong>IDF(Inverse Document Frequency)</strong>则代表该词条在所有文章中的普遍程度。</p><p>​    如果包含词条$t$的文档越少, IDF越大，则说明词条具有很好的类别区分能力。</p><p>​    IDF的公式：</p><script type="math/tex; mode=display">IDF_i=log\frac{|D|}{1+|j:t_i\in d_j|}</script><p>​    其中，$|D|$表示所有文档的数量，$|j:t_i\in d_j|$表示包含词条$t_i$的文档数量，分母加1是为了<strong>防止包含词条$t_i$的文章数量为0导致分母为0</strong></p><h4 id="（3）词频-逆向文件频率（TF-IDF）"><a href="#（3）词频-逆向文件频率（TF-IDF）" class="headerlink" title="（3）词频-逆向文件频率（TF-IDF）"></a>（3）词频-逆向文件频率（TF-IDF）</h4><p>​    某一特定文件内的高词频，以及该词语在所有文件中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于<strong>过滤掉常见的词语，保留重要的词语</strong>，表达为</p><p>​                                                      <strong>TF-IF</strong>$=TF*IDF$</p><hr><h2 id="2-实现"><a href="#2-实现" class="headerlink" title="2.实现"></a>2.实现</h2><h4 id="1）python实现"><a href="#1）python实现" class="headerlink" title="(1）python实现"></a>(1）python实现</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># corpus：语料库</span><span class="token comment"># words：对所有句子分词后的结果，[[], ...]</span><span class="token comment"># word_count：对每个句子进行词频统计， [{}, ...]</span><span class="token comment"># word_dict：每一个句子的词频统计结果， {}</span><span class="token comment"># word：每一个词，word_id</span><span class="token comment"># </span><span class="token keyword">import</span> math<span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict corpus <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">"what is the weather like today"</span><span class="token punctuation">,</span>    <span class="token string">"what is for dinner tonight"</span><span class="token punctuation">,</span>    <span class="token string">"this is a question worth pondering"</span><span class="token punctuation">,</span>    <span class="token string">"it is a beautiful day today"</span><span class="token punctuation">]</span>words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment"># 对corpus分词</span><span class="token keyword">for</span> sentence <span class="token keyword">in</span> corpus<span class="token punctuation">:</span>    words<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sentence<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 如果有自定义的停用词典，我们可以用下列方法来分词并去掉停用词</span><span class="token comment"># f = ["is", "the"]</span><span class="token comment"># for i in corpus:</span><span class="token comment">#     all_words = i.split()</span><span class="token comment">#     new_words = []</span><span class="token comment">#     for j in all_words:</span><span class="token comment">#         if j not in f:</span><span class="token comment">#             new_words.append(j)</span><span class="token comment">#     words.append(new_words)</span><span class="token comment"># print(words)</span> <span class="token comment"># 进行词频统计</span><span class="token keyword">def</span> <span class="token function">Counter</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">:</span>    word_count <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> sentence <span class="token keyword">in</span> words<span class="token punctuation">:</span>        word_dict <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> word <span class="token keyword">in</span> sentence<span class="token punctuation">:</span>            word_dict<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>        word_count<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word_dict<span class="token punctuation">)</span>    <span class="token keyword">return</span> word_count word_count <span class="token operator">=</span> Counter<span class="token punctuation">(</span>words<span class="token punctuation">)</span> <span class="token comment"># 计算TF(word代表被计算的单词，word_dict是被计算单词所在句子分词统计词频后的字典)</span><span class="token keyword">def</span> <span class="token function">tf</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> word_dict<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>word_dict<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 统计含有该单词的句子数</span><span class="token keyword">def</span> <span class="token function">count_sentence</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_count<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> word_count <span class="token keyword">if</span> i<span class="token punctuation">.</span>get<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># i[word] &gt;= 1</span> <span class="token comment"># 计算IDF</span><span class="token keyword">def</span> <span class="token function">idf</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_count<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word_count<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>count_sentence<span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_count<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 计算TF-IDF</span><span class="token keyword">def</span> <span class="token function">tfidf</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_dict<span class="token punctuation">,</span> word_count<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> tf<span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_dict<span class="token punctuation">)</span> <span class="token operator">*</span> idf<span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_count<span class="token punctuation">)</span> p <span class="token operator">=</span> <span class="token number">1</span><span class="token keyword">for</span> word_dict <span class="token keyword">in</span> word_count<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"part:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span>    p <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">for</span> word<span class="token punctuation">,</span> cnt <span class="token keyword">in</span> word_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"word: {} --&gt; TF-IDF:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> tfidf<span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_dict<span class="token punctuation">,</span> word_count<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2）Sklearn实现"><a href="#2）Sklearn实现" class="headerlink" title="(2）Sklearn实现"></a>(2）Sklearn实现</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfTransformer corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'This is the first document.'</span><span class="token punctuation">,</span>       <span class="token string">'This is the second second document.'</span><span class="token punctuation">,</span>       <span class="token string">'And the third one.'</span><span class="token punctuation">,</span>           <span class="token string">'Is this the first document?'</span><span class="token punctuation">]</span> <span class="token comment">#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频</span>vectorizer <span class="token operator">=</span> CountVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#该类会统计每个词语的tf-idf权值</span>tf_idf_transformer <span class="token operator">=</span> TfidfTransformer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#将文本转为词频tf矩阵并计算tf-idf</span>count_vector <span class="token operator">=</span> vectorizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>word <span class="token operator">=</span> vectorizer<span class="token punctuation">.</span>get_feature_names<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span>tfidf <span class="token operator">=</span> tf_idf_transformer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>count_vector<span class="token punctuation">)</span><span class="token comment">#将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重</span><span class="token keyword">print</span><span class="token punctuation">(</span>tfidf<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="3-TF-IDF的应用场景"><a href="#3-TF-IDF的应用场景" class="headerlink" title="3. TF-IDF的应用场景"></a>3. TF-IDF的应用场景</h2><ul><li>搜索引擎</li><li>关键字提取</li><li>文本相似性</li><li>文本摘要</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 大数据分析与计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>西瓜书笔记</title>
      <link href="/2021/09/14/xi-gua-shu-du-shu-bi-ji-1/"/>
      <url>/2021/09/14/xi-gua-shu-du-shu-bi-ji-1/</url>
      
        <content type="html"><![CDATA[<h2 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h2><h3 id="1-2-基本术语"><a href="#1-2-基本术语" class="headerlink" title="1.2 基本术语"></a>1.2 基本术语</h3><ul><li>特征（属性）：反映事件或对象在某方面的表现或性质的事项（如西瓜的”色泽“，”根蒂“，”敲声”)</li><li>属性值：属性上的取值</li><li>属性空间（样本空间、输入空间）：属性张成的空间</li><li>特征向量：属性空间中每个点对应的一个坐标向量</li><li>模型：也称“学习器”</li><li>训练集、验证集、测试集</li><li>泛化能力：学得模型适用于新样本的能力</li></ul><div align="left"><img src="../../themes/matery/source/medias/bp/0.png"></div><h3 id="1-3-假设空间"><a href="#1-3-假设空间" class="headerlink" title="1.3 假设空间"></a>1.3 假设空间</h3><ul><li>版本空间：有多个假设与训练集一致，即存在一个与训练集一致的“假设集合”</li></ul><h3 id="1-4归纳偏好"><a href="#1-4归纳偏好" class="headerlink" title="1.4归纳偏好"></a>1.4归纳偏好</h3><ul><li>“归纳偏好”：机器学习算法在学习过程中对某种类型假设的偏好</li><li><p>最基本的原则—“奥卡姆剃刀”</p></li><li><p>NFL定理：所有学习算法的期望性能一致</p><p><strong>算法的优劣选择要结合具体的问题！！！</strong></p></li></ul><hr><h2 id="第二章-模型的评估与选择"><a href="#第二章-模型的评估与选择" class="headerlink" title="第二章 模型的评估与选择"></a>第二章 模型的评估与选择</h2><p>==👏概要==：基本概念$\Longrightarrow$​模型的评估方法（数据集的划分）$\Longrightarrow$​不同的性能度量指标​</p><h3 id="2-1-经验误差与过拟合"><a href="#2-1-经验误差与过拟合" class="headerlink" title="2.1 经验误差与过拟合"></a>2.1 经验误差与过拟合</h3><p>​    错误率、精度（1-错误率）、误差（训练误差（经验误差）、泛化误差）</p><p>​    过拟合：模型在训练集上训练得太好了，以至于把训练样本自身的一些特点当成所有潜在样本的普遍性质</p><p>​    欠拟合：对训练样本的一般性质尚未学好</p><h3 id="2-2-评估方法"><a href="#2-2-评估方法" class="headerlink" title="2.2 评估方法"></a>2.2 评估方法</h3><h4 id="01-留出法"><a href="#01-留出法" class="headerlink" title="01.留出法"></a>01.留出法</h4><ul><li><p>直接将数据集D按比例划分成两个互斥的集合$(S/T,D=S\cup T,S\cap D=\emptyset)$​</p></li><li><p>通常采用“分层采样”，若干次划分，评估结果取均值</p></li><li><p>存在“偏差-方差”窘境</p><p>测试集小，评估结果方差大；训练集小，评估结果的偏差大</p><p>解决方案：大约2/3~4/5用于训练，剩余用于测试</p></li></ul><h4 id="02-交叉验证法"><a href="#02-交叉验证法" class="headerlink" title="02.交叉验证法"></a>02.交叉验证法</h4><ul><li><p>将数据集D划分为k个大小相似的互斥子集$(D=D_1\cup D_2\cup …\cup D_k,D_i\cap D_j=\emptyset(i\neq j))$​</p></li><li><p>又称为“k折交叉验证”（评估结果的稳定性和保真性很大程度取决于k的取值）</p></li></ul><div align="center"><img src="https://st.blackyau.net/blog/26/5.png" width="40%" height="30%"></div><ul><li>通常需要随机使用不同的划分重复<em>p</em>次，评估结果取均值</li></ul><p>==特例== 留一法：数据集D含有m个样本，k=m,相当于只留出一个样本做测试集</p><p>​                优点:较准确     缺点:计算开销非常大</p><h4 id="03-自助法"><a href="#03-自助法" class="headerlink" title="03.自助法"></a>03.自助法</h4><p>​    每次从数据集$D$​中取一个样本，放回采样，重复m次，得到数据集$D^\prime$​ （ m个训练样本）​,</p><p>​    $D^\prime$做训练集；$D/D^\prime$做测试集($D$中的一部分在$D^\prime$​​​中多次出现，而另一部分样本不出现)</p><p>​        优点：比较适合小的、，难以有效划分训练集/测试集的数据集 </p><p>​                    对集成学习等方法有很大用处</p><p>​        缺点：改变了初始数据集的分布，导致会引入估计偏差</p><h4 id="调参和最终模型"><a href="#调参和最终模型" class="headerlink" title="调参和最终模型"></a>调参和最终模型</h4><p>​    调参：对算法参数进行设定（基于验证集的性能来模型选择和调参）</p><p>​    最终模型：在模型训练完成后，学习算法和参数配置已选定，用完整数据集D重新训练模型，得到最终模型</p><h3 id="2-3-性能度量"><a href="#2-3-性能度量" class="headerlink" title="2.3 性能度量"></a>2.3 性能度量</h3><h4 id="分类问题常用的性能度量"><a href="#分类问题常用的性能度量" class="headerlink" title="分类问题常用的性能度量"></a>分类问题常用的性能度量</h4><h5 id="（1）错误率与精度"><a href="#（1）错误率与精度" class="headerlink" title="（1）错误率与精度"></a>（1）错误率与精度</h5><p>​    <strong>错误率：</strong>分类<u>错误</u>的样本数占样本总数的比例</p><p>​    对离散的样例集D，分类错误率为：</p><script type="math/tex; mode=display">E(f;D)=\frac{1}{m}\sum_{i=1}^m\mathbb I(f(x_i\neq y_i)</script><p>​    对于连续的数据分布D和概率密度p(·)，错误率为：</p><script type="math/tex; mode=display">E(f;D)=\int_{x\sim D}\mathbb I(f(x\neq y)p(x)dx</script><p>​    <strong>精度：</strong>分类<u>正确</u>的样本数占样本总数的比例，精度=1-错误率</p><p>​    对离散的样例集D，分类精度为：</p><script type="math/tex; mode=display">acc(f;D)=\frac{1}{m}\sum_{i=1}^m\mathbb I(f(x_i= y_i)\\        =1-E(f;D)</script><p>​    对于连续的数据分布D和概率密度p(·)，精度为：</p><script type="math/tex; mode=display">acc(f;D)=\int_{x\sim D}\mathbb I(f(x = y)p(x)dx\\        =1-E(f;D)</script><h5 id="（2）准确率、召回率与F1"><a href="#（2）准确率、召回率与F1" class="headerlink" title="（2）准确率、召回率与F1"></a>（2）准确率、召回率与F1</h5><p>​    <strong>混淆矩阵</strong>：也称误差矩阵，是表示精度评价的一种标准格式，  用n行n列的矩阵形式来表示。</p><p>​    对于二分类问题，混淆矩阵如图所示：</p><p><img src="https://img-blog.csdnimg.cn/20200818170300510.png#pic_center" alt=""></p><p>​    <strong>1.准确率：</strong>预测出来为正类中真正的正类所占的比例。</p><script type="math/tex; mode=display">P=\frac{TP}{TP+FP}</script><p>​    <strong>2.召回率：</strong>预测出来正确的正类占所有真实正类的比例。</p><script type="math/tex; mode=display">F=\frac{TP}{TP+FN}</script><p>​    <strong>准确率和召回率是一对矛盾的度量</strong></p><p>​    <strong>3.P-R图</strong>（准确率-召回率曲线)：完全”包住“其他曲线的曲线越优，比如B优于C</p><p><img src="../../themes/matery/source/medias/bp/1.png" alt="P-R图" style="zoom: 70%;"></p><p>​    <strong>4. F1-Score</strong></p><p>​    因为难以比较A和B孰优孰劣，所以有了几个综合考虑准确率、召回率的性能度量</p><p>​    <strong>平衡点(BEP)</strong>：它是”准确率=召回率“时的取值（太过于简化了）</p><div class="table-container"><table><thead><tr><th>F1-Score</th><th></th></tr></thead><tbody><tr><td>基于查准率与查全率的调和平均</td><td>$\frac{1}{F_1}=\frac{1}{2}·(\frac{1}{P}+\frac{1}{F})$​​  即：$F_1=\frac{2\times F\times R}{F+R}=\frac{2\times TP}{样本总数+TP-TN}$​​​​</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">$F_\beta$</th><th>$F_\beta=\frac{1}{1+\beta^2}·(\frac{1}{P}+\frac{\beta^2}{R})$</th></tr></thead><tbody><tr><td style="text-align:left">加权调和平均(表达出对准确率和召回率的不同偏好)</td><td>其中，$\beta$度量了召回率对准确率的相对重要性，$\beta&gt;1$召回率更重要，$\beta&lt;1$准确率更重要</td></tr></tbody></table></div><p>注：<a href="https://www.zhihu.com/question/23096098/answer/513277869">关于调和平均数的理解</a></p><p>很多时候，我们进行多次训练/测试得到多个混淆矩阵，</p><p>此时有两种方法综合考虑准确率和召回率：</p><ol><li><p><strong>”宏F1(macro-F1)”</strong>:算出每个混淆矩阵的准确率和召回率，进行平均,得到宏准确率和宏召回率</p></li><li><p><strong>”微F1(micro-F1)”</strong>:对混淆矩阵的对应元素进行平均，得到TP、FP、TN、FN的平均值，再算微准确率和微召回率</p></li></ol><h5 id="（3）ROC和AUC"><a href="#（3）ROC和AUC" class="headerlink" title="（3）ROC和AUC"></a>（3）ROC和AUC</h5><p>​    <strong>1.ROC</strong></p><p>​    全称是“受试者工作特征”（Receiver Operating Characteristic）曲线，这也是一个用于性能度量的工具，与P-R曲线相似，不过纵轴为“真正例率”（True Positive Rate，TPR），横轴为“假正例率”（False Positive Rate，FPR）</p><ul><li>TPR：真正例 / 真正的正类</li></ul><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script><ul><li><p>FPR：假正例 / 真正的负类</p><script type="math/tex; mode=display">FPR=\frac{FP}{TN+FP}</script></li></ul><p><strong>绘制方法：</strong></p><p>​    预测结果：<br>​        $(s_1, 0.77, +),(s_2, 0.62, −),(s_3, 0.58, +),(s_4, 0.47, +),$</p><p>​        $(s_5, 0.47, −),(s_6, 0.33, −),(s_7, 0.23, +),(s_8, 0.15, −)$</p><p>​        一共4个正类，4个负类$，m_+ = 4，m_- = 4$。</p><p>​    ROC曲线：</p><p><img src="https://img-blog.csdnimg.cn/20200821162529454.png#pic_center" alt=""></p><p>​    注：绿色的线代表该预测结果是个真正例，红色的线代表预测结果是个假正例,蓝色线段代表既新增了真正例也新增了假正例。</p><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">将分类阈值设为最大，TPR和FPR都为0，第一个点(0, 0)；将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例；前一个标记点坐标记为（x， y）；若当前为真正例则对应的标记点坐标为（x, y + 1/m+），若当前为假正例则对应的标记点坐标为（x + 1/m-, y）；最后依次用线段相连相邻节点<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>==性能评价：==</p><p>​    当我们评价多个预测器指标的时候，如果一个预测器的ROC完全<strong>包住</strong>了另外一个预测器的ROC，那么前者性能优于后者，但是如果有<strong>交叉</strong>的部分，就需要比较两者的AUC了。</p><p>​    </p><p><strong>2.AUC</strong></p><p>​        AUC（Area Under ROC Curve），指的是ROC曲线下面的面积，用于评估ROC曲线的性能。</p><script type="math/tex; mode=display">AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1})</script><p>​        注：就是一个梯形面积的公式</p><p><strong>3.排序”损失“（loss)</strong></p><script type="math/tex; mode=display">ℓ_{rank}=\frac{1}{m^+m^-}\sum_{x^+\in D^+}\sum_{x^-\in D^-}(\mathbb I(f(x^+)<f(x^-))+\mathbb I(f(x^+)=f(x^-)))</script><p>​    即考虑每对正、反例，若正例的预测值小于反例，则记一个”罚分“，若相等，记0.5个罚分。</p><p>​    <a href="https://blog.csdn.net/qq_43246110/article/details/104650262">推导过程</a></p><p>​    容易有：$AUC=1-ℓ_{rank}$</p><h5 id="（4）代价敏感错误率与代价曲线"><a href="#（4）代价敏感错误率与代价曲线" class="headerlink" title="（4）代价敏感错误率与代价曲线"></a>（4）代价敏感错误率与代价曲线</h5><p>​    不同类型的错误会造成不同的代价，因此为了权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价“。</p><p>​    二分类问题中</p><p>​    代价矩阵：</p><p><img src="https://pic3.zhimg.com/v2-851291a93a18d269c18f6de3033c36d2_r.jpg" alt=""></p><p>​    ”代价敏感“错误率为：</p><script type="math/tex; mode=display">E(f;D;cost)=\frac{1}{m}(\sum_{x_i \in D^+}func(f(x_i)\neq y_i)*cost_{01}+\sum_{x_i \in D^-}func(f(x_i)\neq y_i)*cost_{10})</script><p>​    在非均等条件下，用<strong>”代价曲线“(Cost Curve)</strong>来反映出模型的期望总体代价</p><p>​    代价曲线的横轴是取值为[0,1]的正例概率代价，其中$p$​是正例概率</p><script type="math/tex; mode=display">\begin{equation}P(+)=\frac{p·cost_{0|1}}{p·cost_{0|1}+(1-p)·cost_{1|0}}    \end{equation}</script><p>​    纵轴是取值为[0,1]的归一化代价</p><script type="math/tex; mode=display">\begin{equation}cost_{norm}=\frac{FNR·p·cost_{0|1}+FPR·(1-p)·cost_{1|0}}{p·cost_{0|1}+(1-p)·cost_{1|0}}\end{equation}</script><p>​    式1代入式2：</p><script type="math/tex; mode=display">cost_{norm}=FNR·P(+)+FPR·(1-P(+))</script><p>​    代价曲线如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-009855d0fa9287349bf4c55964f7160e_720w.jpg?source=1940ef5c" alt="[代价曲线]"></p><p>​    <a href="https://www.zhihu.com/question/63492375">代价曲线的理解？</a></p><h3 id="2-4-比较检验"><a href="#2-4-比较检验" class="headerlink" title="2.4 比较检验"></a>2.4 比较检验</h3><p>​    泛化错误率的分布未知，但可通过测试错误率去推断</p><h4 id="两种最基本的假设检验"><a href="#两种最基本的假设检验" class="headerlink" title="两种最基本的假设检验"></a>两种最基本的假设检验</h4><h5 id="（1）对单个模型泛化性能的假设检验"><a href="#（1）对单个模型泛化性能的假设检验" class="headerlink" title="（1）对单个模型泛化性能的假设检验"></a>（1）对单个模型泛化性能的假设检验</h5><p>​    1.”二项检验“</p><p>​    2.“t检验”(针对多次训练/测试得到多个测试错误率的情况）</p><h5 id="（2）对不同模型的性能比较的假设验证"><a href="#（2）对不同模型的性能比较的假设验证" class="headerlink" title="（2）对不同模型的性能比较的假设验证"></a>（2）对不同模型的性能比较的假设验证</h5><p>​    针对两个算法：</p><p>​    1.交叉验证t检验</p><p>​    2.McNemar检验</p><p>​    针对多个不同的算法：</p><p>​    1.Friedman检验</p><p>​    2.Nemenyi后续检验</p><h3 id="2-5-偏差与方差"><a href="#2-5-偏差与方差" class="headerlink" title="2.5 偏差与方差"></a>2.5 偏差与方差</h3><p><strong>1.泛化误差</strong></p><script type="math/tex; mode=display">E(f,D)=bias^2(x)+var(x)+\varepsilon^2</script><p>​    泛化误差可分解为偏差、方差与噪声之和</p><ul><li><p>偏差：度量了模型的期望预测和真实结果的偏离程度，刻画了<strong>模型本身的拟合能力</strong>。</p></li><li><p>方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即<strong>刻画了数据扰动所造成的影响</strong>。</p></li><li><p>噪声：表达了当前任务上任何模型所能达到的期望泛化误差的下界，<strong>刻画了学习问题本身的难度</strong>。</p></li><li>即泛化性能由算法的学习能力、数据的充分性以及学习任务本身的难度决定的</li></ul><p><strong>2.偏差-方差窘境：</strong></p><p>​    下图给出了在模型训练不足时，拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导泛化误差，此时称为欠拟合现象。当随着训练程度加深，模型的拟合能力增强，训练数据的扰动慢慢使得方差主导泛化误差。当训练充足时，模型的拟合能力非常强，数据轻微变化都能导致模型发生变化，如果过分学习训练数据的特点，则会发生过拟合。</p><ul><li><p>针对欠拟合，我们提出集成学习的概念并且对于模型可以控制训练程度，比如神经网络加多隐层，或者决策树增加树深。<br>增加模型的迭代次数；更换描述能力更强的模型；生成更多特征供训练使用；降低正则化水平。</p></li><li><p>针对过拟合，我们需要降低模型的复杂度，提出了正则化惩罚项。<br>扩增训练集；减少训练使用的特征的数量；提高正则化水平。</p></li></ul><p>若模型复杂度大于平衡点，则模型的方差会偏高，模型倾向于过拟合；若模型复杂度小于平衡点，则模型的偏差会偏高，模型倾向于欠拟合。</p><p><img src="https://images2018.cnblogs.com/blog/606386/201807/606386-20180722194316424-288674381.png" alt="bias-variance-tradeoff"></p><p><a href="https://www.zhihu.com/question/27068705/answer/137487142">机器学习中的 Bias（偏差）、Error（误差）、Variance（方差）有什么区别和联系？</a></p><hr><h2 id="第三章-线性模型"><a href="#第三章-线性模型" class="headerlink" title="第三章 线性模型"></a>第三章 线性模型</h2><p>==👏概要==：线性回归$\Longrightarrow$对数几率回归$\Longrightarrow$线性判别分析$\Longrightarrow$多分类学习$\Longrightarrow$​类别不平衡问题</p><p>补充知识：<a href="https://blog.csdn.net/goodshot/article/details/79953210">向量表示，投影，协方差矩阵_</a></p><p>​                    <a href="https://zhuanlan.zhihu.com/p/37609917">如何直观地理解「协方差矩阵」？ </a></p><h3 id="3-1-回归任务"><a href="#3-1-回归任务" class="headerlink" title="3.1 回归任务"></a>3.1 回归任务</h3><h4 id="（1）线性回归"><a href="#（1）线性回归" class="headerlink" title="（1）线性回归"></a>（1）线性回归</h4><p>​    对于离散属性，若属性值间存在“序”关系，可通过连续化将其转化为连续；若不存在序关系，则通常转化为k维向量。</p><p>​    对于<strong>“多元线性回归”</strong>试图学得：</p><p>​        $f(x_i)=\omega ^T x_i+b,$​使得$f(x_i)\simeq y_i$​</p><p>​    把数据集D表示成一个$m\times(d+1)$​大小的矩阵：</p><script type="math/tex; mode=display">X=\begin{pmatrix} x_{11} & x_{12} & \dots & x_{1d} & 1\\ x_{21} & x_{22} & \dots & x_{2d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots\\ x_{m1} & x_{m2} & \dots & x_{md} & 1 \end{pmatrix} =\begin{pmatrix} x_1^T & 1\\ x_2^T & 1 \\ \vdots & \vdots\\ x_m^T & 1 \end{pmatrix}</script><p>​    把标记也写成向量模式 $y=(y_1;y_2;\dots;y_m)$</p><p>​    有：$\hat{\omega}^*=\mathop{argmin}\limits_{\hat{\omega}}(y-X\hat{\omega})^T(y- X\hat{\omega})$​​（使均方误差最小）</p><p>​    通过最小二乘法对$\omega$和$b$进行估计</p><p>​    若$X^TX$为满秩或正定矩阵，最终学得的多元线性回归模型为：</p><p>​                $f({\hat{x}_i)=\hat{x}_i^T(X^T X)^{-1}X^Ty}$​</p><p>​    但在多数情况下，$X^TX$并不是满秩矩阵，此时可解出多个$\hat{\omega}$​,它们都能使均方误差最小化​，选择哪一个作为输出，由学习算法的归纳偏好决定，常见做法是引入正则项。</p><p><strong>“广义线性模型”</strong>：在形式上仍是线性回归，但实质上已经是在求取输入空间到输出空间的非线性函数映射</p><p>​    考虑单调可微函数$g(·)$，令</p><p>​        $y=g^{-1}(\omega^Tx+b)$​  ，其中$g(·)$称为联系函数</p><p>广义线性模型的参数估计一般通过加权最小二乘法或者极大似然法进行</p><h3 id="3-2-分类任务"><a href="#3-2-分类任务" class="headerlink" title="3.2 分类任务"></a>3.2 分类任务</h3><h4 id="（2）对数几率回归"><a href="#（2）对数几率回归" class="headerlink" title="（2）对数几率回归"></a>（2）对数几率回归</h4><p>​    我们知道线性回归模型预测的是一个值，但如果我们要做的是分类任务呢？</p><p>​    其实就是要找广义线性模型的”联系函数”(link function)，让这个函数将分类任务的真实标记与线性回归的预测值联系起来即可。</p><p>​    考虑二分类任务，其输出标记 $y\in \{0,1\}$ ，而线性模型产生的预测值$z=\omega ^T+b$是实值，于是，我们需将转换$z$为 0/1值，最理想的是”单位阶跃函数”(unit-step function)</p><script type="math/tex; mode=display">y=\left\{\begin{aligned}&0,&z<0\\&\\&0.5,&z=0&&&&&&&&&(1)\\&\\&1,&z>0\end{aligned}\right.</script><p>即若预测值 $z$大于零就判定为正例，小于零，则判定为反例，预测值如果为临界值零则可任意判别，如图1所示：</p><p><img src="https://pic3.zhimg.com/80/v2-20318cc5094a39c05d812653a7ce087a_720w.jpg" alt="图1 单位跃阶函数与对数几率函数" style="zoom:50%;"></p><p>单位阶跃函数不连续（影响后期求最优解[<strong>牛顿法，梯度下降法等</strong>]），于是我们希望找到能一定程度上近似单位阶跃函数的”替代函数”(surrogate function)，并希望它单调可微。对数几率函数(logistic function)正是这样的一个替代函数：</p><script type="math/tex; mode=display">y=\frac{1}{1+e^{-z}}\quad\quad\quad\quad(2)</script><p>对数几率函数是一种”Sigmoid函数”，它将$z$  值转化为一个接近0或1的 $y$ 值，并且在 $z=0$ 附近变化很陡.</p><p>我们把 $z=\omega ^Tx+b$​ 代入到式(2)中，得：</p><script type="math/tex; mode=display">y=\frac{1}{1+e^{-(\omega^Tx+b)}}\quad\quad\quad(3)</script><p>式(3)可变化为:</p><script type="math/tex; mode=display">ln\frac{y}{1-y}=\omega^Tx+b\quad\quad\quad(4)</script><p>若将 $y$​ 视为样本$x$​ 作为正例的可能性，则 $1-y$​ 是其反例的可能性，两者的比值：</p><script type="math/tex; mode=display">\frac{y}{1-y}\quad\quad\quad(5)</script><p>称为”几率”(odds)，反映了$x$作为<strong>正例的相对可能性。</strong>对几率取对数得到<strong>对数几率</strong>(log odds，亦称logit)：</p><script type="math/tex; mode=display">ln\frac{y}{1-y}\quad\quad\quad(6)</script><p>由此可知，式(3)实际是用线性回归模型的预测结果去逼近真实标记的对数几率。因此，其对应的模型简称为”对数几率回归”(logistic regression, 亦称 logit regression)，但实际却是一种分类学习方法。</p><p>这种方法有很多<strong>优点</strong>：</p><p>① 它是直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题；<br>② 它不仅预测类别，也可得到近似概率预测，这对许多需利用概率辅助决策的任务很有用；<br>③ 对率回归求解的目标函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取<strong>最优解</strong>。</p><p>如何确定式(3)的 $w$ 和$b$  ？</p><p>将式(3)中的 $y$​ 视为类后验概率估计 $p=(y=1|x)$​，则式(4)可重写为</p><script type="math/tex; mode=display">ln\frac{p(y=1|x)}{p(y=0|x)}=\omega^Tx+b\quad\quad\quad(7)</script><p>显然有：</p><script type="math/tex; mode=display">p(y=1|x)=\frac{e^{\omega^Tx}+b}{1+e^{\omega^Tx}+b}\quad\quad\quad(8)\\p(y=0|x)=\frac{1}{1+e^{\omega^Tx}+b}\quad\quad\quad(9)</script><p>注： $p(y=1|x)+p(y=0|x)=1$</p><p>于是，我们可通过”极大似然法”（maximum likelihood method）来估计$w$​ 和 $b$​​, 给定数据集${(x_i,y_i)}_{i=1}^m$​  ，对率回归模型最大化”对数似然”（log-likelihood)</p><script type="math/tex; mode=display">\ell(\omega,b)=\sum_{i=1}^mlnp(y_i|x_i;\omega,b)\quad\quad\quad(10)</script><p>即令每个样本属于其真实标记的概率越大越好. 为便于讨论，令$\beta =(\omega;b),\hat{x}=\{x;1\}$​  , 则 $\omega ^Tx+b$​ 可以简写为$\beta ^T\hat{x}$​ .<br>再令$p_1(\hat{x};\beta)=p(y=1|\hat{x};\beta),p_0(\hat{x};\beta)=p(y=0|\hat{x};\beta)=1-p(y=1|\hat{x};\beta)$​  , 则上式中的似然项可重写为：</p><script type="math/tex; mode=display">p(y_i|x_i;\omega,b)=y_ip_1(\hat{x}_i;\beta)+(1-y_i)p_0(\hat{x}_i;\beta)\quad\quad\quad(11)</script><p>注：似然项并不唯一，也可写成$p(y_i|x_i;w)=[p_1(\hat{x_i};\beta)^{y_i}][p_0(\hat{x_i};\beta)^{1-y_i}]$ </p><p>将式(11)代入式(10)，并且由(8)(9)可知，最大化(10)等价于最小化：</p><script type="math/tex; mode=display">\ell(\omega,b)=\sum_{i=1}^m(-y_i\beta^T\hat{x}_i)+ln(1+e^{\beta^T\hat{x}_i}))\quad\quad\quad(12)</script><p>考虑： $y_i\in \{0,1\}$</p><p>式(12)是关于 $\beta $ 的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法(gradient descent method)、牛顿法(Newton method)等都可求得其最优解，于是就得到:</p><script type="math/tex; mode=display">\beta^*=\mathop{arg\ min\ell(\beta)}\limits_\beta\quad\quad\quad(13)</script><p>以牛顿法为例，其第 $t+1$ 轮迭代的更新公式为：</p><script type="math/tex; mode=display">\beta^{t+1}=\beta^t-(\frac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T})^{-1}\frac{\partial\ell(\beta)}{\partial\beta}\quad\quad\quad(14)</script><p>其中关于$\beta $​的一阶、二阶导数分别为：</p><script type="math/tex; mode=display">\frac{\partial\ell(\beta)}{\partial\beta}=-\sum_{i=1}^m\hat{x}_i(y_i-p_1(\hat{x}_i;\beta))\quad\quad\quad(15)\\\frac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T}=\sum_{i=1}^m\hat{x}_i\hat{x}_i^Tp_1(\hat{x}_i;\beta)(1-p_1(\hat{x}_i;\beta))\quad\quad\quad(16)</script><p>​    <a href="https://www.quantinfo.com/Article/View/857.html">对数几率回归 —— Logistic Regression</a></p><h4 id="（3）线性判别分析（LDA）"><a href="#（3）线性判别分析（LDA）" class="headerlink" title="（3）线性判别分析（LDA）"></a>（3）线性判别分析（LDA）</h4><p>​    【线性判别分析（Linear Discriminant Analysis，简称LDA）】是一种经典的【线性学习方法】，在【二分类问题】上因为最早由 Fisher，1936]提出，亦称“Fisher判别分析”</p><p>​    <strong>思想：</strong>给定训练集，设法将样例投影到一条直线上，同类样例的投影点尽可能接近，异类样例的投影点尽可能远离（已有标签的投影，有监督的学习到这条直线).</p><p><img src="https://img2020.cnblogs.com/blog/1163900/202011/1163900-20201129064736038-699456951.png" alt="" style="zoom: 33%;"></p><p>​    对新样本分类时，将其投影到同样的这条直线上，根据投影点的位置来确定新样本的类别。</p><p>​    <strong>因为特征向量的个数通常要远远少于原有数据特征的个数，因此线性判别分析也被视为一种经典的监督降维技术。</strong></p><p>​    <a href="https://www.cnblogs.com/pinard/p/6244265.html">线性判别分析LDA原理总结</a></p><h3 id="3-3-多分类学习"><a href="#3-3-多分类学习" class="headerlink" title="3.3 多分类学习"></a>3.3 多分类学习</h3><h4 id="1-基本思路"><a href="#1-基本思路" class="headerlink" title="1.基本思路"></a>1.基本思路</h4><p>“拆解法”：将多分类任务拆分为若干个二分类问题</p><h4 id="2-拆分策略"><a href="#2-拆分策略" class="headerlink" title="2. 拆分策略"></a>2. 拆分策略</h4><ul><li><p>“一对一”（OvO)：将N个类别两两配对，从而产生$N(N-1)/2$个分类任务</p><p>投票产生最终结果(即把被预测得最多的类别作为最终分类结果)</p></li><li><p>“一对其余”（OvR)：每次将一个类的样例作为正例、其他所有分类的样例作为反例来训练N个分类器</p><p>仅有一个分类器预测为正类，则对应类别标记为最终分类结果；若多个分类器预测为正类，则选择置信度最大的类别标记作为分类结果</p></li><li><p>“多对多”（MvM)：每次将若干类作为正类，若干其它类作为反类</p><p>常用方法：</p><p><strong>ECOC(‘’纠错输出码’’)</strong></p><p>二元码（”+1“，”-1“分别表示正、反例）和三元码（比二元码多一个”0“停用类）</p></li></ul><h3 id="3-4-类别不平衡问题"><a href="#3-4-类别不平衡问题" class="headerlink" title="3.4 类别不平衡问题"></a>3.4 类别不平衡问题</h3>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 周志华西瓜书 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的新世界</title>
      <link href="/2021/09/14/hello-world/"/>
      <url>/2021/09/14/hello-world/</url>
      
        <content type="html"><![CDATA[<h1 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h1><p>​    花了半天新搞了一个博客，在这里主要是为了分享分享学习笔记，督促自己学习。</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
